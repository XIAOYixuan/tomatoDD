model:
    model_class: Wav2Vec2Model
    activation_dropout: 0.1
    activation_fn: gelu
    adam_betas: (0.9,0.98)
    adam_eps: 1.0e-06
    arch: wav2vec2
    attention_dropout: 0.0
    attention_type: default
    augment: false
    best_checkpoint_metric: loss
    bpe: null
    bucket_cap_mb: 25
    centroids: null
    clip_norm: 25
    codebook_negatives: 0
    combine_banks: false
    conv_bias: false
    conv_feature_layers: '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2'
    conv_pos: 128
    conv_pos_groups: 16
    conv_pos_layers: 1
    cpu: false
    criterion: wav2vec
    cross_sample_negatives: 0
    curriculum: 0
    dataset_impl: null
    ddp_backend: c10d
    device_id: 0
    disable_validation: false
    distributed_backend: nccl
    distributed_init_method: tcp://learnfair2106:55498
    distributed_no_spawn: true
    distributed_port: 55498
    distributed_rank: 0
    distributed_world_size: 64
    div_drop_percent: 0
    div_pen_threshold: null
    dropout: 0.0
    dropout_features: 0.1
    dropout_input: 0
    duplicate_negatives: 0
    empty_cache_freq: 0
    enable_padding: false
    encode_padded_indiv: false
    encoder_attention_heads: 12
    encoder_embed_dim: 768
    encoder_ffn_embed_dim: 3072
    encoder_layerdrop: 0.1
    encoder_layers: 12
    encoder_normalize_before: true
    end_learning_rate: 0.0
    extractor_mode: default
    extractor_model: null
    extractor_norm_location: default
    fast_stat_sync: false
    feature_glu: false
    feature_grad_mult: 1.0
    feature_noise: 0.0
    feature_noise_last: 0.0
    features_pen: true
    final_dim: 256
    find_unused_parameters: true
    finetune_extractor: true
    fix_batches_to_gpus: false
    fixed_validation_seed: null
    force_anneal: null
    fp16: true
    fp16_init_scale: 128
    fp16_scale_tolerance: 0.0
    fp16_scale_window: null
    group_norm_features: false
    group_norm_groups: 512
    gumbel_noise_gain: 1
    infomax: true
    input_noise: 0.0
    keep_interval_updates: 1
    keep_last_epochs: -1
    label_smoothing: 0.0
    labels: null
    latent_dim: 0
    latent_groups: 2
    latent_temp: (2,0.5,0.999995)
    latent_var_banks: 2
    latent_vars: 320
    layer_norm_after: 9223372036854775807
    layer_norm_before: 0
    layer_norm_features: true
    layer_norm_first: false
    lazy_load_labels: false
    log_format: json
    log_interval: 200
    logit_temp: 0.1
    loss_weights: null
    lr:
    - 0.0005
    lr_scheduler: polynomial_decay
    mask_channel_length: 64
    mask_channel_min_space: 1
    mask_channel_other: 0.0
    mask_channel_prob: 0.5
    mask_channel_selection: static
    mask_length: 10
    mask_min_space: 1
    mask_multiple_length: 10
    mask_other: 0.0
    mask_prob: 0.65
    mask_same_channels: false
    mask_same_timesteps: false
    mask_selection: static
    mask_stdev: 0.0
    masking_schedule: 0
    max_epoch: 0
    max_positions: 8000
    max_pred_length: 0
    max_sample_size: 250000
    max_sentences: null
    max_sentences_valid: null
    max_tokens: 1400000
    max_tokens_valid: 1400000
    max_update: 400000
    maximize_best_checkpoint_metric: false
    memory_efficient_fp16: false
    min_loss_scale: 0.0001
    min_lr: -1
    min_sample_size: 32000
    mlp_mi: 256
    negatives_from_everywhere: false
    new_emb_pen: true
    new_logit_pen: false
    no_bert_init: false
    no_epoch_checkpoints: true
    no_last_checkpoints: false
    no_mask_channel_overlap: false
    no_mask_overlap: false
    no_norm_after: 0
    no_progress_bar: false
    no_save: false
    no_save_optimizer_state: false
    no_token_positional_embeddings: true
    noise_type: gaussian
    norm_init_weight: 1.0
    normalize: false
    num_negatives: 100
    num_workers: 6
    optimizer: adam
    optimizer_overrides: '{}'
    penalty_coeff: '[0, 0, 0.1, 10]'
    penalty_temp: 1.0
    pooler_activation_fn: tanh
    pooler_dropout: 0.0
    power: 1.0
    pre_norm: false
    predictor_grad_mult: 1.0
    preemp: false
    project_quantized: true
    quantize_input: false
    quantize_targets: true
    quantized: false
    quantizer_chance: 0.0
    quantizer_grad_mult: 1.0
    quantizer_init: true
    quantizer_init_gain: 1.0
    quantizer_init_normal: true
    relative_positional_embeddings: 0
    required_batch_size_multiple: 8
    resample_method: linear
    rescale_sample_size: false
    reset_dataloader: false
    reset_lr_scheduler: false
    reset_meters: false
    reset_optimizer: false
    restore_file: checkpoint_last.pt
    same_quantizer: false
    sample_rate: 16000
    save_dir: /checkpoint/abaevski/asr/speechbert_raw_250k_q5/best_ld_400k.nep.qtz.nnf0.ng512.pq.lv320.lvb2.lr0.0005.mask10.mprob0.65.mstd0.mstd0.05.drp_i0.1.drp_f0.1.fgm0.1.qini1.fpen.pen0_0_0.1_10.cpl1.neg100.mxsz250000.s5.ngpu64
    save_interval: 1
    save_interval_updates: 25000
    scp: false
    seed: 5
    sentence_avg: false
    siamese_extractor: false
    siamese_feature_layers: null
    skip_connections: false
    skip_invalid_size_inputs_valid_test: true
    skip_main_loss_prob: 0
    soft: false
    squeeze_constant: 20
    squeeze_logits: norm_temp
    squeeze_pos_emb: add
    squeeze_quantizer_logits: false
    static_preemp: false
    tanh_after_norm: 0.0
    target_glu: false
    task: audio_pretraining
    tensorboard_logdir: ''
    threshold_loss_scale: null
    tokenizer: null
    total_num_update: 400000
    train_on_full: false
    train_subset: train
    unprojected_feats: false
    update_freq:
    - 1
    use_aggregator_feats: false
    use_bmuf: false
    use_old_adam: false
    user_dir: null
    valid_subset: valid
    validate_after_updates: 0
    validate_interval: 5
    validate_interval_updates: 10000
    warmup_updates: 32000
    weight_decay: 0.01
    weight_norm: false
